<div id="readme" class="Box-body readme blob js-code-block-container p-5 p-xl-6 gist-border-0">
    <article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-google-analytics-customer-revenue-analysis" class="anchor" aria-hidden="true" href="#google-analytics-customer-revenue-analysis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Google Analytics Customer Revenue Analysis</h1>
<h2><a id="user-content-table-of-content" class="anchor" aria-hidden="true" href="#table-of-content"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Table of Content</h2>
<ul>
<li><a href="#group-member">Group Member</a></li>
<li><a href="#executive-summary">Executive Summary</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#results-and-conclusions">Results and Conclusions</a></li>
<li><a href="#challenges">Challenges</a></li>
<li><a href="#futurework">Future Work</a></li>
<li><a href="#takeaways-from-the-course">Takeaways From the Course</a></li>
</ul>
<h2><a id="user-content-group-member" class="anchor" aria-hidden="true" href="#group-member"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Group Member</h2>
<ul>
<li>Zili Bu</li>
<li>Hsin-Yu Chen</li>
<li>Huang-Chin Yen</li>
<li>Kexin Zhang</li>
</ul>
<h2><a id="user-content-executive-summary" class="anchor" aria-hidden="true" href="#executive-summary"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Executive Summary</h2>
<h3><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h3>
<p>This is the data we obtained from Kaggle analysis. In this competition, the original challenge was to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. However, we are planning for some new challenges form this Working with Large Dataset Class.</p>
<h3><a id="user-content-the-problems" class="anchor" aria-hidden="true" href="#the-problems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The problems</h3>
<ul>
<li>Can we deal with this large dataset with similar methods as we used in small datasets?</li>
<li>What can we do to our json column?</li>
<li>What model will be appropriate to this certain project?</li>
<li>Should we include all the variables in this dataset to do prediction about total revenues?</li>
</ul>
<h3><a id="user-content-the-solutions" class="anchor" aria-hidden="true" href="#the-solutions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The solutions</h3>
<ul>
<li>We will use both what we learned in class and also our experience from dealing with small datasets in this project.</li>
<li>We would try to convert the json column into regular data frame. If it does not work, we would delete the column since there are a lot of NAs in it.</li>
<li>We will first try to use linear regression model to fit our data because the dependent variable is numerical. We would also try some model such as XGBoost, Random Forest, and Gradient Boost.</li>
<li>We will make the decision later after we look through the coefficient matrix and the important features.</li>
</ul>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introduction</h2>
<p>We found Google Analytics Customer Revenue Prediction dataset from Kaggle. <a href="https://www.kaggle.com/c/ga-customer-revenue-prediction/overview" rel="nofollow">Dataset Link</a></p>
<p>Our dataset is sources from Google Analytics platform. It contains 12 columns with 4 json format columns with 1708337 rows of data. After the conversion, it has 60 columns instead:</p>
<ul>
<li>bool(1)</li>
<li>int64(4)</li>
<li>object(55))</li>
</ul>
<p>The Column Information</p>
<ul>
<li>fullVisitorId- A unique identifier for each user of the Google Merchandise Store.</li>
<li>channelGrouping - The channel via which the user came to the Store.</li>
<li>date - The date on which the user visited the Store.</li>
<li>device - The specifications for the device used to access the Store.</li>
<li>geoNetwork - This section contains information about the geography of the user.</li>
<li>socialEngagementType - Engagement type, either "Socially Engaged" or "Not Socially Engaged".</li>
<li>totals - This section contains aggregate values across the session.</li>
<li>trafficSource - This section contains information about the Traffic Source from which the session originated.</li>
<li>visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.</li>
<li>visitNumber - The session number for this user. If this is the first session, then this is set to 1.</li>
<li>visitStartTime - The timestamp (expressed as POSIX time).</li>
<li>hits - This row and nested fields are populated for any and all types of hits. Provides a record of all page visits.</li>
<li>customDimensions - This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set.</li>
<li>totals - This set of columns mostly includes high-level aggregate data.</li>
</ul>
<p>As the website introduced, the 80/20 rule is a prevalent rule for many businesses in the real world. It is important to understand our target customers and attract their willingness to purchase the product at Google Store by coordinating proper marketing promotions.</p>
<p>Here are some important business questions:</p>
<ul>
<li>Which region/continent has the highest customers, or highest revenues?</li>
<li>Which type of devices/OS has the highest access?</li>
<li>Which channel group has the highest access or revenues?</li>
<li>Which weekdays, months have the highest access or revenue?</li>
<li>Can we build some models to predict the revenues from our customers?</li>
</ul>
<p>After the initial exploration about our data, we would like to focus our analysis on columns like:</p>
<ul>
<li><code>totals.transactionRevenue</code>: The revenue made from this visit.</li>
<li><code>device.browser</code>: which browser is this visit used.</li>
<li><code>device.deviceCategory</code> : what kind of device is this visit (tablet, mobile or desktop).</li>
<li><code>channelGrouping</code>: The channel via which the user came to the Store. (organic search, display ads, direct, etc.)</li>
<li><code>device.operatingSystem</code>: which operating system is this visit.</li>
</ul>
<h2><a id="user-content-results-and-conclusions" class="anchor" aria-hidden="true" href="#results-and-conclusions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Results and Conclusions</h2>
<ul>
<li>
<p>From our analysis, we found out that total hits and visit hour as well as visit numbers are important features when predicting whether the visitor's total transaction revenue is high or low.</p>
</li>
<li>
<p>In conclusion, in this dataset, majority of the visitors have total hits smaller than 100 and spent more than 10 hours. It is reasonable since most of the people would think twice and browsing around the website before buying items online. Moreover, most of the visitors who spend higher are not in the first session. It might because visitors who spend more have already been through the website before or have already browsing around the website many time before making transaction.</p>
</li>
<li>
<p>We randomly split the data into train and test sets. We utilize test accuracy or test error to evaluate our model performance.</p>
</li>
</ul>
<h2><a id="user-content-challenges" class="anchor" aria-hidden="true" href="#challenges"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Challenges</h2>
<ol>
<li>
<p>The very first challenge is how to handle with these json type columns in our csv file. Fortunately, we find a code provided by another analyst in Kaggle to help us transform our data. <a href="https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields" rel="nofollow">reference</a> However, the kernel failed everytime we convert the json column, therefore, we finish the conversion on local machine and save the converted file into S3 for further analysis.</p>
</li>
<li>
<p>The introduction of this dataset on Kaggle.com is not very clear. Therefore, we need to spend some time on the internet to look into the dataset to understand the meaning of each column, especially those new columns converted from json.</p>
</li>
<li>
<p>We have a very large dataset, and it takes a long time to process (the kernel even died several times) . Therefore, we use a smaller sample of our original dataset to ensure the correctness of our code. After that, we apply all the code on the larger dataset to save our time.</p>
</li>
<li>
<p>The json columns after conversion have a period in the column name, which hinder our <code>spark.sql</code> command later on. Then we realize that we need to use a back quote symbol to quote those column names to perform sql command.</p>
</li>
<li>
<p>Column <code>totals.transactionRevenue</code> has a lot of null value after converted from json, we have to fill all the null value with zero since it means they do not conduct any transactions through these visits.</p>
</li>
<li>
<p>Our dataset contains many null values and most of the features are categorical, and even many numerical features are constant value, which is challenging for model building. After cleaning up the null value and label the categorical feature, our model accuracy enhanced from <strong>28%</strong> to <strong>54%</strong>.</p>
</li>
<li>
<p>Most of the value in our original target prediction "total.transactionrevenue" are 0, which is somehow meaningless for prediction. Therefore, instead of building regression models, we split the revenue into three classes and build <code>classifier models</code> for predictions.</p>
</li>
</ol>
<h2><a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Future Work</h2>
<ol>
<li>
<p>We do not have much information about the <code>groupchannels</code> (what are their differences, their return on investment, etc). If we could have more data about these channels, we can evaluate which social platform or referral methods generates more vitis to Gstore. As a result, we can reallocate the resources to different social and referral channels to optimize the results. For example, to improve the poor performing ones or to expand the size of the good perfoming ones.</p>
</li>
<li>
<p>Our model of the classifying level of revenue of customers has a best accuracy <strong>54%</strong>, which remains a lot of space for improvement. Instead of directly deleting the null value and constant data, we can try to gather more information and take more references to better deal with those missing values, which might enhanced model prediction by considering those deleted features. Also, instead of labelizing categorical variables, we may try one-hot-encoding method to check whether it is better.</p>
</li>
</ol>
<h2><a id="user-content-takeaways-from-the-course" class="anchor" aria-hidden="true" href="#takeaways-from-the-course"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Takeaways From the Course</h2>
<ul>
<li>We are able to get familiar with <strong>github</strong>, for example, how to connect github, create and clone repositories, and how to update our work to a repo in github. Meanwhile, we also benefit from the github's ability to record the history(changes) of each commits we made.</li>
<li>The practise of <strong>regular express</strong> help us to extract desired string from certain text files.</li>
<li>The experience to work with AWS platform is great. We learned how to properly connect to a virtual machine, and navigate through our virtual machine and extract informations about our files on<code>HDFS</code> or <code>S3</code> file system via shell command.</li>
<li>We get a brief introduction on <code>map</code> and <code>reduce</code> function when we dealing with large size of data in the real world. Meanwhile, we also experience the power of the parallized computations in clusters on how to increase the speed of processing tasks.</li>
<li>Establish a spark environment to run <code>spark.sql</code> command directly in python (juypter notebook)</li>
<li>When using spark dataframe, we realize the <code>.cache()</code> method could save the dataframe into memory so that we are able to save a lot of time to run sql command towards that spark dataframe, especially with such a large size of data.</li>
<li>Build machine learning models through pipeline.</li>
</ul>
</article>
  </div>
